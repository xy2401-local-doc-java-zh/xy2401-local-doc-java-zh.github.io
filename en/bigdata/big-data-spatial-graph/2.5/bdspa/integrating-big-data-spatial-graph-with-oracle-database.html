<!DOCTYPE html
  SYSTEM "about:legacy-compat">
<html xml:lang="en-us" lang="en-us">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1">
      <meta http-equiv="X-UA-Compatible" content="IE=edge">
      <meta name="abstract" content="You can use Oracle Big Data Connectors to facilitate spatial data access between Big Data Spatial and Graph and Oracle Database.">
      <meta name="description" content="You can use Oracle Big Data Connectors to facilitate spatial data access between Big Data Spatial and Graph and Oracle Database.">
      <title>Integrating Big Data Spatial and Graph with Oracle Database</title>
      <meta property="og:site_name" content="Oracle Help Center">
      <meta property="og:title" content="User’s Guide and Reference">
      <meta property="og:description" content="You can use Oracle Big Data Connectors to facilitate spatial data access between Big Data Spatial and Graph and Oracle Database.">
      <link rel="stylesheet" href="/sp_common/book-template/ohc-book-template/css/book.css">
      <link rel="shortcut icon" href="/sp_common/book-template/ohc-common/img/favicon.ico">
      <meta name="application-name" content="User’s Guide and Reference">
      <meta name="generator" content="DITA Open Toolkit version 1.8.5 (Mode = doc)">
      <meta name="plugin" content="SP_docbuilder HTML plugin release 18.2.2">
      <link rel="alternate" href="oracle-big-data-spatial-and-graph-users-guide-and-reference.pdf" title="PDF File" type="application/pdf">
      <meta name="robots" content="all">
      <link rel="schema.dcterms" href="http://purl.org/dc/terms/">
      <meta name="dcterms.created" content="2018-06-08T14:08:33-07:00">
      
      <meta name="dcterms.dateCopyrighted" content="2015, 2018">
      <meta name="dcterms.category" content="bigdata">
      <meta name="dcterms.identifier" content="E67958-15">
      
      <meta name="dcterms.product" content="en/bigdata/big-data-spatial-graph/2.5">
      
      <link rel="prev" href="using-big-data-spatial-graph-spatial-data.html" title="Previous" type="text/html">
      <link rel="next" href="configuring-property-graph-support.html" title="Next" type="text/html">
      <script>
        document.write('<style type="text/css">');
        document.write('body > .noscript, body > .noscript ~ * { visibility: hidden; }');
        document.write('</style>');
     </script>
      <script data-main="/sp_common/book-template/ohc-book-template/js/book-config" src="/sp_common/book-template/requirejs/require.js"></script>
      <script>
            if (window.require === undefined) {
                document.write('<script data-main="sp_common/book-template/ohc-book-template/js/book-config" src="sp_common/book-template/requirejs/require.js"><\/script>');
                document.write('<link href="sp_common/book-template/ohc-book-template/css/book.css" rel="stylesheet"/>');
            }
        </script>
      <script type="application/json" id="ssot-metadata">{"primary":{"category":{"short_name":"bigdata","element_name":"Big Data","display_in_url":true},"suite":{"short_name":"not-applicable","element_name":"Not Applicable","display_in_url":false},"product_group":{"short_name":"not-applicable","element_name":"Not applicable","display_in_url":false},"product":{"short_name":"big-data-spatial-graph","element_name":"Big Data Spatial and Graph","display_in_url":true},"release":{"short_name":"2.5","element_name":"Release 2.5","display_in_url":true}}}</script>
      
    <meta name="dcterms.title" content="Oracle Big Data Spatial and Graph User's Guide and Reference">
    <meta name="dcterms.isVersionOf" content="BDSPA">
    <meta name="dcterms.release" content="Release 2.5">
  <script>window.ohcglobal || document.write('<script src="/en/dcommon/js/global.js">\x3C/script>')</script></head>
   <body>
      <div class="noscript alert alert-danger text-center" role="alert">
         <a href="using-big-data-spatial-graph-spatial-data.html" class="pull-left"><span class="glyphicon glyphicon-chevron-left" aria-hidden="true"></span>Previous</a>
         <a href="configuring-property-graph-support.html" class="pull-right">Next<span class="glyphicon glyphicon-chevron-right" aria-hidden="true"></span></a>
         <span class="fa fa-exclamation-triangle" aria-hidden="true"></span> JavaScript must be enabled to correctly display this content
        
      </div>
      <article>
         <header>
            <ol class="breadcrumb" vocab="http://schema.org/" typeof="BreadcrumbList">
               <li property="itemListElement" typeof="ListItem"><a href="index.html" property="item" typeof="WebPage"><span property="name">User’s Guide and Reference</span></a></li>
               <li class="active" property="itemListElement" typeof="ListItem">Integrating Big Data Spatial and Graph with Oracle Database</li>
            </ol>
            <a id="GUID-77B38F53-21C9-4147-BE88-211A57E8A413" name="GUID-77B38F53-21C9-4147-BE88-211A57E8A413"></a>
            
            <h2 id="BDSPA-GUID-77B38F53-21C9-4147-BE88-211A57E8A413" class="sect2"><span class="enumeration_chapter">3 </span>Integrating Big Data Spatial and Graph with Oracle Database
            </h2>
         </header>
         <div class="ind">
            <div>
               <p>You can use Oracle Big Data Connectors to facilitate spatial data access between Big Data Spatial and Graph and Oracle Database.</p>
               <p>This chapter assumes that you have a working knowledge of the following:</p>
               <ul style="list-style-type: disc;">
                  <li>
                     <p>Oracle SQL Connector for HDFS</p>
                     <p>For information, see <a href="https://www.oracle.com/pls/topic/lookup?ctx=en/bigdata/big-data-spatial-graph/2.5/bdspa&amp;id=BDCUG125" target="_blank">Oracle SQL Connector for Hadoop Distributed File System</a>.
                     </p>
                  </li>
                  <li>
                     <p>Oracle Loader for Hadoop </p>
                     <p>For information, see <a href="https://www.oracle.com/pls/topic/lookup?ctx=en/bigdata/big-data-spatial-graph/2.5/bdspa&amp;id=BDCUG140" target="_blank">Oracle Loader for Hadoop</a></p>
                  </li>
                  <li>
                     <p>Apache Hive</p>
                     <p>For information, see the Apache Hive documentation at <a href="https://cwiki.apache.org/confluence/display/Hive/Home#Home-UserDocumentation" target="_blank">https://cwiki.apache.org/confluence/display/Hive/Home#Home-UserDocumentation</a>.
                     </p>
                  </li>
               </ul>
            </div>
            <div>
               <ul class="ullinks">
                  <li class="ulchildlink"><a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-DAC1D06C-D054-4C0B-B2F7-6BC5EEDFD287">Using Oracle SQL Connector for HDFS with Delimited Text Files</a><br>This topic is applicable when the files in HDFS are delimited text files (fields must be delimited using single-character markers, such as commas or tabs) <span class="bold">and</span> the spatial data is stored as GeoJSON or WKT format.
                  </li>
                  <li class="ulchildlink"><a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-0A6EC67C-36A2-4480-A420-7363AC6E31F1">Using Oracle SQL Connector for HDFS with Hive Tables</a><br>Oracle SQL Connector for HDFS (OSCH) directly supports HIVE tables defined on HDFS.
                  </li>
                  <li class="ulchildlink"><a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-F6BE6A07-F94F-4347-85E3-10818A48CBAA">Using Oracle SQL Connector for HDFS with Files Generated by Oracle Loader for Hadoop</a><br>To use Oracle SQL Connector for HDFS (OSCH) with files generated by Oracle Loader for Hadoop (OLH), you must understand how OLH is used to move data from HDFS to Oracle Database.
                  </li>
                  <li class="ulchildlink"><a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-22143F85-39AA-4C28-8B57-D588AA1D9D8A">Integrating HDFS Spatial Data with Oracle Database Using Oracle Big Data SQL</a><br>You can use Oracle Big Data SQL to facilitate spatial data access between HDFS and Oracle Database.
                  </li>
               </ul>
            </div>
            
            <div class="props_rev_3"><a id="GUID-DAC1D06C-D054-4C0B-B2F7-6BC5EEDFD287" name="GUID-DAC1D06C-D054-4C0B-B2F7-6BC5EEDFD287"></a><h3 id="BDSPA-GUID-DAC1D06C-D054-4C0B-B2F7-6BC5EEDFD287" class="sect3"><span class="enumeration_section">3.1 </span>Using Oracle SQL Connector for HDFS with Delimited Text Files
               </h3>
               <div>
                  <p>This topic is applicable when the files in HDFS are delimited text files (fields must be delimited using single-character markers, such as commas or tabs) <span class="bold">and</span> the spatial data is stored as GeoJSON or WKT format.
                  </p>
                  <p>If such data is to be used by Big Data Spatial and Graph and is to be accessed from an Oracle database using the Oracle SQL connection for HDFS, certain configuration steps are needed.</p>
                  <p>For this example, assume that the files in HDFS contain records separated by new lines, and the fields within each record are separated by tabs, such as in the following:</p><pre class="oac_no_warn" dir="ltr">"6703"	1	62	"Hong Kong"	3479846	POINT (114.18306 22.30693)
"6702"	57	166	"Singapore"	1765655	POINT (103.85387 1.29498) </pre><ol>
                     <li>
                        <p>Log in to a node of the Hadoop cluster.</p>
                     </li>
                     <li>
                        <p>Create the configuration file required by OSCH (Oracle SQL Connector for HDFS), such as the following example:</p><pre class="oac_no_warn" dir="ltr">&lt;?xml version="1.0"?&gt;
 &lt;configuration&gt;
    &lt;property&gt;
      &lt;name&gt;oracle.hadoop.exttab.tableName&lt;/name&gt;
      &lt;value&gt;TWEETS_EXT_TAB_FILE&lt;/value&gt; 
    &lt;/property&gt; 
   &lt;property&gt;
      &lt;name&gt;oracle.hadoop.exttab.sourceType&lt;/name&gt;
      &lt;value&gt;text&lt;/value&gt;
    &lt;/property&gt;
   &lt;property&gt; 
      &lt;name&gt;oracle.hadoop.exttab.dataPaths&lt;/name&gt;
      &lt;value&gt;/user/scott/simple_tweets_data/*.log&lt;/value&gt; 
    &lt;/property&gt;   
    &lt;property&gt;
      &lt;name&gt;oracle.hadoop.connection.url&lt;/name&gt;
      &lt;value&gt;jdbc:oracle:thin:@//myhost:1521/myservicename&lt;/value&gt; 
    &lt;/property&gt; 
    &lt;property&gt;
      &lt;name&gt;oracle.hadoop.connection.user&lt;/name&gt;
      &lt;value&gt;scott&lt;/value&gt; 
    &lt;/property&gt;      
    &lt;property&gt;
      &lt;name&gt;oracle.hadoop.exttab.fieldTerminator&lt;/name&gt;
      &lt;value&gt;\u0009&lt;/value&gt; 
    &lt;/property&gt;      
    &lt;property&gt;
      &lt;name&gt;oracle.hadoop.exttab.columnNames&lt;/name&gt;
      &lt;value&gt;ID,FOLLOWERS_COUNT,FRIENDS_COUNT,LOCATION,USER_ID,GEOMETRY&lt;/value&gt; 
    &lt;/property&gt;      
    &lt;property&gt;
      &lt;name&gt;oracle.hadoop.exttab.defaultDirectory&lt;/name&gt;
      &lt;value&gt;TWEETS_DT_DIR&lt;/value&gt; 
    &lt;/property&gt;      
&lt;/configuration&gt;
</pre></li>
                     <li>
                        <p>Name the configuration file <code class="codeph">tweets_text.xml</code>.
                        </p>
                     </li>
                     <li>
                        <p>On a node of the Hadoop cluster, execute the following command:</p><pre class="pre codeblock"><code>hadoop jar $OSCH_HOME/jlib/orahdfs.jar \
       oracle.hadoop.exttab.ExternalTable \
       -conf /home/oracle/tweets_text.xml \
       -createTable
</code></pre><p>The command prompts for the database password .</p>
                        <p>You can either create the OSCH_HOME environment variable or replace OSCH_HOME in the command syntax with the full path to the installation directory for Oracle SQL Connector for HDFS. On Oracle Big Data Appliance, this directory is: <code class="codeph">/opt/oracle/orahdfs-version</code></p>
                     </li>
                  </ol>
                  <p>The table TWEETS_EXT_TAB_FILE is now ready to query. It can be queried like any other table from the database. The database is the target database specified in the configuration file in a previous step.. The following query selects the count of rows in the table:</p><pre class="pre codeblock"><code>select count(*) from TWEETS_EXT_TAB_FILE;</code></pre><p>You can perform spatial operations on that table just like any other spatial table in the database. The following example retrieves information about users that are tweeting within in a quarter-mile (0.25 mile) radius of a specific movie theater:</p><pre class="pre codeblock"><code>select sdo_geom.SDO_DISTANCE(ci.geometry, SDO_GEOMETRY(tw.geometry, 8307), 0.05, 'UNIT=MILE'), ci.name, tw.user_id 
from CINEMA ci, TWEETS_EXT_TAB_FILE tw where SDO_WITHIN_DISTANCE(ci.geometry, SDO_GEOMETRY(tw.geometry, 8307), 'DISTANCE=0.25 UNIT=MILE') = 'TRUE'
</code></pre><p>Here the table CINEMA is a spatial table in the Oracle database, and the HDFS table TWEETS_EXT_TAB_FILE can be used to query against this table. The data from the tweets table is read in as WKT (well known text), and the WKT constructor of SDO_GEOMETRY is used to materialize this data as a geometry in the database.</p>
                  <p>Note that the SRID of the geometries is 8307. Also ,if the spatial data is in GeoJSON format, then the query should be as follows:</p><pre class="pre codeblock"><code>select sdo_geom.SDO_DISTANCE(ci.geometry, SDO_UTIL.FROM_GEOJSON(tw.geometry, '', 8307), 0.05, 'UNIT=MILE'), ci.name, tw.user_id 
from CINEMA ci, TWEETS_EXT_TAB_FILE tw where SDO_WITHIN_DISTANCE(ci.geometry, SDO_UTIL.FROM_GEOJSON(tw.geometry, '', 8307), 'DISTANCE=0.25 UNIT=MILE') = 'TRUE'
</code></pre></div>
               <div>
                  <div class="familylinks">
                     <div class="parentlink">
                        <p><strong>Parent topic:</strong> <a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-77B38F53-21C9-4147-BE88-211A57E8A413" title="You can use Oracle Big Data Connectors to facilitate spatial data access between Big Data Spatial and Graph and Oracle Database.">Integrating Big Data Spatial and Graph with Oracle Database</a></p>
                     </div>
                  </div>
               </div>
               
            </div>
            <div class="props_rev_3"><a id="GUID-0A6EC67C-36A2-4480-A420-7363AC6E31F1" name="GUID-0A6EC67C-36A2-4480-A420-7363AC6E31F1"></a><h3 id="BDSPA-GUID-0A6EC67C-36A2-4480-A420-7363AC6E31F1" class="sect3"><span class="enumeration_section">3.2 </span>Using Oracle SQL Connector for HDFS with Hive Tables
               </h3>
               <div>
                  <p>Oracle SQL Connector for HDFS (OSCH) directly supports HIVE tables defined on HDFS.</p>
                  <p>The Hive tables must be nonpartitioned, and defined using&nbsp;ROW FORMAT DELIMITED&nbsp;and&nbsp;FILE FORMAT TEXTFILE&nbsp;clauses.&nbsp;The spatial data must be in GeoJSON or WKT format.</p>
                  <p>Both Hive-managed tables and Hive external tables are supported.</p>
                  <p>For example, the Hive command to create a table on the file described in <a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-DAC1D06C-D054-4C0B-B2F7-6BC5EEDFD287" title="This topic is applicable when the files in HDFS are delimited text files (fields must be delimited using single-character markers, such as commas or tabs) and the spatial data is stored as GeoJSON or WKT format.">Using Oracle SQL Connector for HDFS with Delimited Text Files</a> is as follows. It assumes that the user already has a Hive table defined on HDFS data. The data in HDFS must be in the supported format, and the spatial data must be in GeoJSON or WKT format. 
                  </p><pre class="pre codeblock"><code>CREATE EXTERNAL TABLE IF NOT EXISTS TWEETS_HIVE_TAB(
  ID string, 
  FOLLOWERS_COUNT int, 
  FRIENDS_COUNT int, 
  LOCATION string, 
  USER_ID int, 
  GEOMETRY string)
ROW FORMAT DELIMITED 
  FIELDS TERMINATED BY '\t' 
STORED AS INPUTFORMAT 
  'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 
  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
  '/user/scott/simple_tweets_data';
</code></pre><p>The following example queries the table.</p><pre class="pre codeblock"><code>select ID, FOLLOWERS_COUNT, FRIENDS_COUNT, LOCATION, USER_ID, GEOMETRY from TWEETS_HIVE_TAB limit 10;</code></pre><p>The output looks as follow:</p><pre class="oac_no_warn" dir="ltr">"6703"	1	62	"Hong Kong"	3479846	POINT (114.18306 22.30693)
"6702"	57	166	"Singapore"	1765655	POINT (103.85387 1.29498)
</pre><ol>
                     <li>
                        <p>Log in to a node of the Hadoop cluster.</p>
                     </li>
                     <li>
                        <p>Create the configuration file required by OSCH (Oracle SQL Connector for HDFS), such as the following example:</p><pre class="oac_no_warn" dir="ltr">&lt;?xml version="1.0"?&gt;
 &lt;configuration&gt;
    &lt;property&gt;
      &lt;name&gt;oracle.hadoop.exttab.tableName&lt;/name&gt;
      &lt;value&gt;TWEETS_EXT_TAB_HIVE&lt;/value&gt; 
    &lt;/property&gt; 
   &lt;property&gt; 
      &lt;name&gt;oracle.hadoop.exttab.sourceType&lt;/name&gt;
      &lt;value&gt;hive&lt;/value&gt; 
    &lt;/property&gt;   
   &lt;property&gt; 
      &lt;name&gt;oracle.hadoop.exttab.hive.tableName&lt;/name&gt;
      &lt;value&gt;TWEETS_HIVE_TAB&lt;/value&gt; 
    &lt;/property&gt;   
   &lt;property&gt; 
      &lt;name&gt;oracle.hadoop.exttab.hive.databaseName&lt;/name&gt;
      &lt;value&gt;default&lt;/value&gt; 
    &lt;/property&gt;   
    &lt;property&gt;
      &lt;name&gt;oracle.hadoop.connection.url&lt;/name&gt;
      &lt;value&gt;jdbc:oracle:thin:@//myhost:1521/myservicename&lt;/value&gt; 
    &lt;/property&gt; 
    &lt;property&gt;
      &lt;name&gt;oracle.hadoop.connection.user&lt;/name&gt;
      &lt;value&gt;scott&lt;/value&gt; 
    &lt;/property&gt;      
    &lt;property&gt;
      &lt;name&gt;oracle.hadoop.exttab.defaultDirectory&lt;/name&gt;
      &lt;value&gt;TWEETS_DT_DIR&lt;/value&gt; 
    &lt;/property&gt;      
&lt;/configuration&gt; 
</pre></li>
                     <li>
                        <p>Name the configuration file <code class="codeph">tweets_text.xml</code>.
                        </p>
                     </li>
                     <li>
                        <p>On a node of the Hadoop cluster, execute the following command:</p><pre class="pre codeblock"><code># Add HIVE_HOME/lib* to HADOOP_CLASSPATH.  
export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HIVE_HOME/lib/*
hadoop jar $OSCH_HOME/jlib/orahdfs.jar \
       oracle.hadoop.exttab.ExternalTable \
       -conf /home/oracle/tweets_hive.xml \
       -createTable
</code></pre><p>The command prompts for the database password . You can either create the OSCH_HOME environment variable or replace OSCH_HOME in the command syntax with the full path to the installation directory for Oracle SQL Connector for HDFS. On Oracle Big Data Appliance, this directory is: <code class="codeph">/opt/oracle/orahdfs-version</code></p>
                        <p>Set the environment variable HIVE_HOME to point to the Hive installation directory (for example, <code class="codeph">/usr/lib/hive</code>).
                        </p>
                     </li>
                  </ol>
                  <p>The table TWEETS_EXT_TAB_FILE is now ready to query. It can be queried like any other table from the database. The following query selects the count of rows in the table:</p><pre class="pre codeblock"><code>select count(*) from TWEETS_EXT_TAB_HIVE;;</code></pre><p>You can perform spatial operations on that table just like any other spatial table in the database. The following example retrieves information about users that are tweeting within in a quarter-mile (0.25 mile) radius of a specific movie theater:</p><pre class="pre codeblock"><code>select sdo_geom.SDO_DISTANCE(ci.geometry, SDO_GEOMETRY(tw.geometry, 8307), 0.05, 'UNIT=MILE), ci.name, tw.user_id 
from CINEMA ci, TWEETS_EXT_TAB_HIVE tw where SDO_WITHIN_DISTANCE(ci.geometry, SDO_GEOMETRY(tw.geometry, 8307), 'DISTANCE=0.25 UNIT=MILE') = 'TRUE'
</code></pre><p>Here the table CINEMA is a spatial table in the Oracle database, and the HDFS table TWEETS_EXT_TAB_FILE can be used to query against this table. The data from the tweets table is read in as WKT (well known text), and the WKT constructor of SDO_GEOMETRY is used to materialize this data as a geometry in the database.</p>
                  <p>Note that the SRID of the geometries is 8307. Also ,if the spatial data is in GeoJSON format, then the query should be as follows:</p><pre class="pre codeblock"><code>select sdo_geom.SDO_DISTANCE(ci.geometry, SDO_UTIL.FROM_GEOJSON(tw.geometry, '', 8307), 0.05, 'UNIT=MILE), ci.name, tw.user_id 
from CINEMA ci, TWEETS_EXT_TAB_HIVE tw where SDO_WITHIN_DISTANCE(ci.geometry, SDO_UTIL.FROM_GEOJSON(tw.geometry, '', 8307), 'DISTANCE=0.25 UNIT=MILE') = 'TRUE'
</code></pre></div>
               <div>
                  <div class="familylinks">
                     <div class="parentlink">
                        <p><strong>Parent topic:</strong> <a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-77B38F53-21C9-4147-BE88-211A57E8A413" title="You can use Oracle Big Data Connectors to facilitate spatial data access between Big Data Spatial and Graph and Oracle Database.">Integrating Big Data Spatial and Graph with Oracle Database</a></p>
                     </div>
                  </div>
               </div>
               
            </div>
            <div class="props_rev_3"><a id="GUID-F6BE6A07-F94F-4347-85E3-10818A48CBAA" name="GUID-F6BE6A07-F94F-4347-85E3-10818A48CBAA"></a><h3 id="BDSPA-GUID-F6BE6A07-F94F-4347-85E3-10818A48CBAA" class="sect3"><span class="enumeration_section">3.3 </span>Using Oracle SQL Connector for HDFS with Files Generated by Oracle Loader for Hadoop
               </h3>
               <div>
                  <p>To use Oracle SQL Connector for HDFS (OSCH) with files generated by Oracle Loader for Hadoop (OLH), you must understand how OLH is used to move data from HDFS to Oracle Database.</p>
                  <p>Modifications are required for moving Big Data Spatial and Graph spatial data into the database. This solution generally applies for any kind of files in HDFS or any kind of Hive data. The spatial information can be in a well known format or a custom format. </p>
                  <p>First, an example of how to create external tables from files in HDFS containing spatial information in a user defined format. Assume that the files in HDFS have records the following format:</p><pre class="oac_no_warn" dir="ltr">{
	"type":"Feature",
	"id":"6703",
	"followers_count":1,
	"friends_count":62,
	"location":"Hong Kong",
	"user_id":3479846,
	"longitude":114.18306,
	"latitude":22.30693
}

{
	"type":"Feature",
	"id":"6702",
	"followers_count":57,
	"friends_count":166,
	"location":"Singapore",
	"user_id":1765655,
	"longitude":103.85387,
	"latitude":1.29498
}
</pre><p>The Hive command to create a table for those records is as follows:</p><pre class="pre codeblock"><code>add jar
     /opt/oracle/oracle-spatial-graph/spatial/vector/jlib/ojdbc8.jar
     /opt/oracle/oracle-spatial-graph/spatial/vector/jlib/sdoutl.jar
     /opt/oracle/oracle-spatial-graph/spatial/vector/jlib/sdoapi.jar
     /opt/oracle/oracle-spatial-graph/spatial/vector/jlib/sdohadoop-vector.jar
     /opt/oracle/oracle-spatial-graph/spatial/vector/jlib/sdohadoop-vector-hive.jar
     <span class="bold">… (add here jars containing custom SerDe and/or InputFormats);</span>
CREATE EXTERNAL TABLE IF NOT EXISTS CUST_TWEETS_HIVE_TAB (id STRING, geometry STRING, followers_count STRING, friends_count STRING, location STRING, user_id STRING)                                         
ROW FORMAT SERDE 'mypackage.TweetsSerDe'              
STORED AS INPUTFORMAT 'oracle.spatial.hadoop.vector.geojson.mapred.GeoJsonInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION '/user/scott/simple_tweets_data';
</code></pre><p>The <code class="codeph">InputFormat</code> object <code class="codeph">oracle.spatial.hadoop.vector.geojson.mapred.GeoJsonInputFormat</code> can read those records even if they are not strict GeoJSON. Thus, the preceding example does not need a custom <code class="codeph">InputFormat</code> specification. However, it does require a custom Hive Serializer and Deserializer (SerDe) to transform the latitude and longitude into a WKT or GeoJSON geometry. For that, the Spatial Java API can be used in the deserialize function of the SerDe, as the following example
                  </p><pre class="pre codeblock"><code>    @Override
    public Object deserialize(Writable w) throws SerDeException {
        Text rowText = (Text) w;
        List&lt;Text&gt; row = new ArrayList&lt;Text&gt;(columnNames.size());
        
        //default all values to null
        for(int i=0;i&lt;columnNames.size();i++){
        	row.add(null);
        }
        
        // Try parsing row into JSON object
        JsonNode recordNode = null;
        
        try {
        	String txt = rowText.toString().trim();
        	recordNode = jsonMapper.readTree(txt);
			row.set(columnNames.indexOf("id"), new Text(recordNode.get("id").getTextValue()));
			row.set(columnNames.indexOf("followers_count"), new Text(recordNode.get("followers_count").toString()));
			row.set(columnNames.indexOf("friends_count"), new Text(recordNode.get("friends_count").toString()));
			row.set(columnNames.indexOf("location"), new Text(recordNode.get("location").getTextValue()));
			row.set(columnNames.indexOf("user_id"), new Text(recordNode.get("user_id").toString()));
			
			Double longitude = recordNode.get("longitude").getDoubleValue();
			Double latitude = recordNode.get("latitude").getDoubleValue();
			
			//use the Spatial API to create the geometry
			JGeometry geom = JGeometry.createPoint(new double[]{
					longitude, 
					latitude}, 
					2, //dimensions
					8307 //SRID
					);
			//Transform the JGeometry to WKT
 			String geoWKT = new String(wkt.fromJGeometry(geom));
			row.set(columnNames.indexOf("geometry"), new Text(geoWKT));
        } catch (Exception e) {
            throw new SerDeException("Exception parsing JSON: " +e.getMessage(), e);
        }
	
        return row;
    }    
</code></pre><p>In the preceding example, to return the geometries in GeoJSON format, replace the following:</p><pre class="pre codeblock"><code>String geoWKT = new String(wkt.fromJGeometry(geom));
row.set(columnNames.indexOf("geometry"), new Text(geoWKT));
</code></pre><p>with this:</p><pre class="pre codeblock"><code>row.set(columnNames.indexOf("geometry"), new Text(geom.toGeoJson()));</code></pre><p>More SerDe examples to transform data in GeoJSON, WKT, or ESRI Shapefiles with the Spatial Java API are available in the folder: <code class="codeph">/opt/oracle/oracle-spatial-graph/spatial/vector/examples/hive/java/src/oracle/spatial/hadoop/vector/hive/java/src/serde</code></p>
                  <p>The following example queries the Hive table:</p><pre class="pre codeblock"><code>select ID, FOLLOWERS_COUNT, FRIENDS_COUNT, LOCATION, USER_ID, GEOMETRY from CUST_TWEETS_HIVE_TAB limit 10;</code></pre><p>The output looks like the following:</p><pre class="oac_no_warn" dir="ltr">6703	1	62	Hong Kong	3479846	POINT (114.18306 22.30693)
6702	57	166	Singapore	1765655	POINT (103.85387 1.29498)
</pre></div>
               <div>
                  <ul class="ullinks">
                     <li class="ulchildlink"><a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-42654E7F-F712-43C3-8AE8-2CDD5601F878">Creating HDFS Data Pump Files or Delimited Text Files</a><br></li>
                     <li class="ulchildlink"><a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-CB769408-088C-40B0-B246-7FB73CC9B534">Creating the SQL Connector for HDFS</a><br></li>
                  </ul>
                  <div class="familylinks">
                     <div class="parentlink">
                        <p><strong>Parent topic:</strong> <a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-77B38F53-21C9-4147-BE88-211A57E8A413" title="You can use Oracle Big Data Connectors to facilitate spatial data access between Big Data Spatial and Graph and Oracle Database.">Integrating Big Data Spatial and Graph with Oracle Database</a></p>
                     </div>
                  </div>
               </div>
               
               <div class="props_rev_3"><a id="GUID-42654E7F-F712-43C3-8AE8-2CDD5601F878" name="GUID-42654E7F-F712-43C3-8AE8-2CDD5601F878"></a><h4 id="BDSPA-GUID-42654E7F-F712-43C3-8AE8-2CDD5601F878" class="sect4"><span class="enumeration_section">3.3.1 </span>Creating HDFS Data Pump Files or Delimited Text Files
                  </h4>
                  <div>
                     <p>You can use the Hive table from <a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-F6BE6A07-F94F-4347-85E3-10818A48CBAA" title="To use Oracle SQL Connector for HDFS (OSCH) with files generated by Oracle Loader for Hadoop (OLH), you must understand how OLH is used to move data from HDFS to Oracle Database.">Using Oracle SQL Connector for HDFS with Files Generated by Oracle Loader for Hadoop</a> to create HDFS Data Pump files or delimited text files. 
                     </p>
                     <p></p>
                     <ol>
                        <li>
                           <p>Create a table in the Oracle database as follows:</p><pre class="pre codeblock"><code>CREATE TABLE tweets_t(id INTEGER
  PRIMARY KEY, geometry VARCHAR2(4000), followers_count NUMBER,
  friends_count NUMBER, location VARCHAR2(4000), user_id NUMBER);
</code></pre><p>This table will be used as the target table. Oracle Loader for Hadoop uses table metadata from the Oracle database to identify the column names, data types, partitions, and other information. For simplicity, create this table with the same columns (fields) as the Hive table. After the external table is created, you can remove this table or use it to insert the rows from the external table into the target table. (For more information about target tables, see <a href="https://www.oracle.com/pls/topic/lookup?ctx=en/bigdata/big-data-spatial-graph/2.5/bdspa&amp;id=BDCUG462" target="_blank">About the Target Table Metadata</a>.
                           </p>
                        </li>
                        <li>
                           <p>Create the loader configuration file, as in the following example:</p><pre class="oac_no_warn" dir="ltr">&lt;?xml version="1.0" encoding="UTF-8" ?&gt;
&lt;configuration&gt;
&lt;!--                          Input settings                             --&gt;
&lt;property&gt;
	&lt;name&gt;mapreduce.inputformat.class&lt;/name&gt;
	&lt;value&gt;oracle.hadoop.loader.lib.input.HiveToAvroInputFormat&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
	&lt;name&gt;oracle.hadoop.loader.input.hive.databaseName&lt;/name&gt;
	&lt;value&gt;default&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
	&lt;name&gt;oracle.hadoop.loader.input.hive.tableName&lt;/name&gt;
	&lt;value&gt;CUST_TWEETS_HIVE_TAB&lt;/value&gt;
&lt;/property&gt;
&lt;!--                          Output settings                             --&gt;
 &lt;property&gt;
   &lt;name&gt;mapreduce.outputformat.class&lt;/name&gt;
   &lt;value&gt;oracle.hadoop.loader.lib.output.DataPumpOutputFormat&lt;/value&gt;
 &lt;/property&gt;
 &lt;property&gt;
   &lt;name&gt;mapred.output.dir&lt;/name&gt;
   &lt;value&gt;/user/scott/data_output&lt;/value&gt;
 &lt;/property&gt;
&lt;!--                          Table information                            --&gt;
&lt;property&gt;
	&lt;name&gt;oracle.hadoop.loader.loaderMap.targetTable&lt;/name&gt;
	&lt;value&gt;tweets_t&lt;/value&gt;
&lt;/property&gt; 
&lt;!--                          Connection information                      --&gt;
&lt;property&gt;
  &lt;name&gt;oracle.hadoop.loader.connection.url&lt;/name&gt;
  &lt;value&gt;jdbc:oracle:thin:@//myhost:1521/myservicename&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;oracle.hadoop.loader.connection.user&lt;/name&gt;
    &lt;value&gt;scott&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
    &lt;name&gt;oracle.hadoop.loader.connection.password&lt;/name&gt;
    &lt;value&gt;welcome1&lt;/value&gt;        
    &lt;description&gt; Having the password in cleartext is NOT RECOMMENDED. Use Oracle Wallet instead. &lt;/description&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</pre><p>With this configuration, Data Pump files will be created in HDFS. If you want delimited text files as the output, then replace th following:</p><pre class="oac_no_warn" dir="ltr">oracle.hadoop.loader.lib.output.DataPumpOutputFormat</pre><p>with this:</p><pre class="oac_no_warn" dir="ltr">oracle.hadoop.loader.lib.output.DelimitedTextOutputFormat</pre></li>
                        <li>
                           <p>Name the configuration file <code class="codeph">tweets_hive_to_data_pump.xml</code>.
                           </p>
                        </li>
                        <li>
                           <p>Create the Data Pump files:</p><pre class="pre codeblock"><code># Add HIVE_HOME/lib* and the Hive configuration directory to HADOOP_CLASSPATH.
export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HIVE_HOME/lib/*:$HIVE_CONF_DIR
# Add Oracle Spatial libraries to HADOOP_CLASSPATH.
export ORACLE_SPATIAL_VECTOR_LIB_PATH=/opt/oracle/oracle-spatial-graph/spatial/vector/jlib

export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$ORACLE_SPATIAL_VECTOR_LIB_PATH/ojdbc8.jar:$ORACLE_SPATIAL_VECTOR_LIB_PATH/sdoutl.jar:$ORACLE_SPATIAL_VECTOR_LIB_PATH/sdoapi.jar:$ORACLE_SPATIAL_VECTOR_LIB_PATH/sdohadoop-vector.jar:$ORACLE_SPATIAL_VECTOR_LIB_PATH/sdohadoop-vector-hive.jar

# The Oracle Spatial libraries need to be added to the libjars option as well.
export LIBJARS=$ORACLE_SPATIAL_VECTOR_LIB_PATH/ojdbc8.jar,$ORACLE_SPATIAL_VECTOR_LIB_PATH/sdoutl.jar,$ORACLE_SPATIAL_VECTOR_LIB_PATH/sdoapi.jar,$ORACLE_SPATIAL_VECTOR_LIB_PATH/sdohadoop-vector.jar,$ORACLE_SPATIAL_VECTOR_LIB_PATH/sdohadoop-vector-hive.jar

# And the following HIVE jar files have to be added to the libjars option.
export LIBJARS=$LIBJARS,$HIVE_HOME/lib/hive-exec-*.jar,$HIVE_HOME/lib/hive-metastore-*.jar,$HIVE_HOME/lib/libfb303*.jar

hadoop jar ${OLH_HOME}/jlib/oraloader.jar \
           oracle.hadoop.loader.OraLoader \
           -conf /home/oracle/tweets_hive_to_data_pump.xml \
           -libjars $LIBJARS
</code></pre></li>
                     </ol>
                     <p>For the preceding example:</p>
                     <ul style="list-style-type: disc;">
                        <li>
                           <p>Be sure that the environment variable OLH_HOME has to be set to the installation directory.</p>
                        </li>
                        <li>
                           <p>Set the environment variable HIVE_HOME to point to the Hive installation directory (for example, <code class="codeph">/usr/lib/hive</code>).
                           </p>
                        </li>
                        <li>
                           <p>Set the environment variable HIVE_CONF_DIR to point to the Hive configuration directory (for example, <code class="codeph">/etc/hive/conf</code>).
                           </p>
                        </li>
                        <li>
                           <p>Add the following Hive jar files, in a comma-separated list, to the <code class="codeph">-libjars</code> option of the <code class="codeph">hadoop</code> command. Replace the asterisks (*) with the complete file names on your system:
                           </p><pre class="oac_no_warn" dir="ltr">hive-exec-*.jar
hive-metastore-*.jar
libfb303*.jar
</pre></li>
                        <li>
                           <p>If <code class="codeph">oracle.kv.hadoop.hive.table.TableStorageHandler</code> is used to create the Hive table (with the data coming from Oracle NoSQL Database), you must also add the following jar file to the <code class="codeph">-libjars</code> option of the <code class="codeph">hadoop</code> command: <code class="codeph">$KVHOME/lib/kvclient.jar</code> (where KVHOME is the directory where the Oracle NoSQL Database is installed)
                           </p>
                        </li>
                        <li>
                           <div class="p">If <code class="codeph">org.apache.hadoop.hive.hbase.HBaseStorageHandler</code> is used to create the Hive table (with the data coming from Apache HBase), you must also add the following JAR files, in a comma-separated list, to the <code class="codeph">-libjars</code> option of the <code class="codeph">hadoop</code> command:<pre class="oac_no_warn" dir="ltr">$HIVE_HOME/lib/hbase-server.jar
$HIVE_HOME/lib/hive-hbase-handler.jar
$HIVE_HOME/lib/hbase-common.jar
$HIVE_HOME/lib/hbase-client.jar
$HIVE_HOME/lib/hbase-hadoop2-compat.jar
$HIVE_HOME/lib/hbase-hadoop-compat.jar
$HIVE_HOME/lib/hbase-protocol.jar
$HIVE_HOME/lib/htrace-core.jar
</pre></div>
                        </li>
                     </ul>
                  </div>
                  <div>
                     <div class="familylinks">
                        <div class="parentlink">
                           <p><strong>Parent topic:</strong> <a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-F6BE6A07-F94F-4347-85E3-10818A48CBAA" title="To use Oracle SQL Connector for HDFS (OSCH) with files generated by Oracle Loader for Hadoop (OLH), you must understand how OLH is used to move data from HDFS to Oracle Database.">Using Oracle SQL Connector for HDFS with Files Generated by Oracle Loader for Hadoop</a></p>
                        </div>
                     </div>
                  </div>
                  
               </div>
               <div class="props_rev_3"><a id="GUID-CB769408-088C-40B0-B246-7FB73CC9B534" name="GUID-CB769408-088C-40B0-B246-7FB73CC9B534"></a><h4 id="BDSPA-GUID-CB769408-088C-40B0-B246-7FB73CC9B534" class="sect4"><span class="enumeration_section">3.3.2 </span>Creating the SQL Connector for HDFS
                  </h4>
                  <div>
                     <p>To create the SQL Connector fo HDFS, follow the instructions in this topic.</p>
                     <ol>
                        <li>
                           <p>Create the configuration file for the SQL Connector for HDFS), as in the following example:</p><pre class="oac_no_warn" dir="ltr">&lt;?xml version="1.0"?&gt;
 &lt;configuration&gt;
    &lt;property&gt;
      &lt;name&gt;oracle.hadoop.exttab.tableName&lt;/name&gt;
      &lt;value&gt;TWEETS_EXT_TAB_DP&lt;/value&gt; 
    &lt;/property&gt; 
    &lt;property&gt;
      &lt;name&gt;oracle.hadoop.exttab.sourceType&lt;/name&gt;
      &lt;value&gt;datapump&lt;/value&gt; 
    &lt;/property&gt; 
   &lt;property&gt; 
      &lt;name&gt;oracle.hadoop.exttab.dataPaths&lt;/name&gt;
      &lt;value&gt;/user/scott/data_output/oraloader-0000*.dat&lt;/value&gt;
    &lt;/property&gt;   
    &lt;property&gt;
      &lt;name&gt;oracle.hadoop.connection.url&lt;/name&gt;
      &lt;value&gt;jdbc:oracle:thin:@//myhost:1521/myservicename&lt;/value&gt; 
    &lt;/property&gt; 
    &lt;property&gt;
      &lt;name&gt;oracle.hadoop.connection.user&lt;/name&gt;
      &lt;value&gt;scott&lt;/value&gt; 
    &lt;/property&gt;      
    &lt;property&gt;
      &lt;name&gt;oracle.hadoop.exttab.defaultDirectory&lt;/name&gt;
      &lt;value&gt;TWEETS_DT_DIR&lt;/value&gt; 
    &lt;/property&gt;   
&lt;/configuration&gt;
</pre><p>If the files are delimited text files, follow the steps in <a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-DAC1D06C-D054-4C0B-B2F7-6BC5EEDFD287" title="This topic is applicable when the files in HDFS are delimited text files (fields must be delimited using single-character markers, such as commas or tabs) and the spatial data is stored as GeoJSON or WKT format.">Using Oracle SQL Connector for HDFS with Delimited Text Files</a>.
                           </p>
                        </li>
                        <li>
                           <p>Name the configuration file <code class="codeph">tweets_ext_from_dp.xml</code>.
                           </p>
                        </li>
                        <li>
                           <p>Create the external table.</p><pre class="pre codeblock"><code>hadoop jar $OSCH_HOME/jlib/orahdfs.jar \
           oracle.hadoop.exttab.ExternalTable \
           -conf /home/oracle/tweets_ext_from_dp.xml\
           -createTable
</code></pre><p>In the preceding command, you can either create the OSCH_HOME environment variable, or replace OSCH_HOME in the command with the full path to the installation directory for Oracle SQL Connector for HDFS. On Oracle Big Data Appliance, this directory is: <code class="codeph">/opt/oracle/orahdfs-version</code></p>
                        </li>
                     </ol>
                     <p>The table TWEETS_EXT_TAB_DP is now ready to query. It can be queried like any other table in the database. For example:</p><pre class="pre codeblock"><code>select count(*) from TWEETS_EXT_TAB_DP;</code></pre><p>You can perform spatial operations on that table, such as the following example to retrieve the users that are tweeting in a quarter-mile radius of a cinema:</p><pre class="pre codeblock"><code>select sdo_geom.SDO_DISTANCE(ci.geometry, SDO_GEOMETRY(tw.geometry, 8307), 0.5, 'UNIT=YARD'), ci.name, tw.user_id 
from CINEMA ci, TWEETS_EXT_TAB_DP tw where SDO_WITHIN_DISTANCE(ci.geometry, SDO_GEOMETRY(tw.geometry, 8307), 'DISTANCE=200 UNIT=MILE') = 'TRUE';
</code></pre><p>This information can be used further to customize advertising.</p>
                     <p>Note that the SRID of the geometries is 8307. Also, if the spatial data is in GeoJSON format, then the query should be as follows:</p><pre class="pre codeblock"><code>select sdo_geom.SDO_DISTANCE(ci.geometry, SDO_UTIL.FROM_GEOJSON(tw.geometry, '', 8307), 0.5, 'UNIT=YARD'), ci.name, tw.user_id 
from CINEMA ci, TWEETS_EXT_TAB_DP tw where SDO_WITHIN_DISTANCE(ci.geometry, SDO_UTIL.FROM_GEOJSON(tw.geometry, '', 8307), 'DISTANCE=200 UNIT=MILE') = 'TRUE';
</code></pre></div>
                  <div>
                     <div class="familylinks">
                        <div class="parentlink">
                           <p><strong>Parent topic:</strong> <a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-F6BE6A07-F94F-4347-85E3-10818A48CBAA" title="To use Oracle SQL Connector for HDFS (OSCH) with files generated by Oracle Loader for Hadoop (OLH), you must understand how OLH is used to move data from HDFS to Oracle Database.">Using Oracle SQL Connector for HDFS with Files Generated by Oracle Loader for Hadoop</a></p>
                        </div>
                     </div>
                  </div>
                  
               </div>
            </div>
            <div class="props_rev_3"><a id="GUID-22143F85-39AA-4C28-8B57-D588AA1D9D8A" name="GUID-22143F85-39AA-4C28-8B57-D588AA1D9D8A"></a><h3 id="BDSPA-GUID-22143F85-39AA-4C28-8B57-D588AA1D9D8A" class="sect3"><span class="enumeration_section">3.4 </span>Integrating HDFS Spatial Data with Oracle Database Using Oracle Big Data SQL
               </h3>
               <div>
                  <p>You can use Oracle Big Data SQL to facilitate spatial data access between HDFS and Oracle Database.</p>
                  <p>To enable the spatial features in Oracle Big Data SQL, update the file <code class="codeph">bigdata.properties</code> to add the following lines at the end (replacing $ORACLE_SPATIAL_VECTOR_LIB_PATH with the path to the Oracle Spatial libraries):
                  </p><pre class="oac_no_warn" dir="ltr">java.classpath.user=$ORACLE_SPATIAL_VECTOR_LIB_PATH/ojdbc8.jar:
$ORACLE_SPATIAL_VECTOR_LIB_PATH/sdoutl.jar: $ORACLE_SPATIAL_VECTOR_LIB_PATH/sdoapi.jar:
$ORACLE_SPATIAL_VECTOR_LIB_PATH/sdohadoop-vector.jar:
$ORACLE_SPATIAL_VECTOR_LIB_PATH/sdohadoop-vector-hive.jar
(Also add here jars containing custom SerDe and/or InputFormat specifications.)
</pre><p>If the files are in HDFS, you can use the following solutions: </p>
                  <ul style="list-style-type: disc;">
                     <li>
                        <p><a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-5C2C58CE-84BA-42E7-BB99-011B366B5DA8">Creating Oracle External Tables for HDFS Files with Big Data SQL</a> 
                        </p>
                     </li>
                     <li>
                        <p><a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-7E915C08-D008-4750-8377-C7AD89C1FD88">Creating Oracle External Tables Using Hive Tables with Big Data SQL</a></p>
                     </li>
                  </ul>
                  <p>If you are accessing spatial data from Oracle NoSQL Database or Apache HBase, you can use the solution in <a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-7E915C08-D008-4750-8377-C7AD89C1FD88">Creating Oracle External Tables Using Hive Tables with Big Data SQL</a>.
                  </p>
                  <p></p>
                  <p></p>
                  <p>To use Oracle SQL Connector for HDFS (OSCH) with files generated by Oracle Loader for Hadoop (OLH), you must understand how OLH is used to move data from HDFS to Oracle Database.</p>
                  <p>Modifications are required for moving Big Data Spatial and Graph spatial data into the database. This solution generally applies for any kind of files in HDFS or any kind of Hive data. The spatial information can be in a well known format or a custom format. </p>
                  <p>First, an example of how to create external tables from files in HDFS containing spatial information in a user defined format. Assume that the files in HDFS have records the following format:</p><pre class="oac_no_warn" dir="ltr">{
	"type":"Feature",
	"id":"6703",
	"followers_count":1,
	"friends_count":62,
	"location":"Hong Kong",
	"user_id":3479846,
	"longitude":114.18306,
	"latitude":22.30693
}

{
	"type":"Feature",
	"id":"6702",
	"followers_count":57,
	"friends_count":166,
	"location":"Singapore",
	"user_id":1765655,
	"longitude":103.85387,
	"latitude":1.29498
}
</pre><p>The Hive command to create a table for those records is as follows:</p><pre class="pre codeblock"><code>add jar
     /opt/oracle/oracle-spatial-graph/spatial/vector/jlib/ojdbc8.jar
     /opt/oracle/oracle-spatial-graph/spatial/vector/jlib/sdoutl.jar
     /opt/oracle/oracle-spatial-graph/spatial/vector/jlib/sdoapi.jar
     /opt/oracle/oracle-spatial-graph/spatial/vector/jlib/sdohadoop-vector.jar
     /opt/oracle/oracle-spatial-graph/spatial/vector/jlib/sdohadoop-vector-hive.jar
     <span class="bold">… (add here jars containing custom SerDe and/or InputFormats);</span>
CREATE EXTERNAL TABLE IF NOT EXISTS CUST_TWEETS_HIVE_TAB (id STRING, geometry STRING, followers_count STRING, friends_count STRING, location STRING, user_id STRING)                                         
ROW FORMAT SERDE 'mypackage.TweetsSerDe'              
STORED AS INPUTFORMAT 'oracle.spatial.hadoop.vector.geojson.mapred.GeoJsonInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION '/user/scott/simple_tweets_data';
</code></pre><p>The <code class="codeph">InputFormat</code> object <code class="codeph">oracle.spatial.hadoop.vector.geojson.mapred.GeoJsonInputFormat</code> can read those records even if they are not strict GeoJSON. Thus, the preceding example does not need a custom <code class="codeph">InputFormat</code> specification. However, it does require a custom Hive Serializer and Deserializer (SerDe) to transform the latitude and longitude into a WKT or GeoJSON geometry. For that, the Spatial Java API can be used in the deserialize function of the SerDe, as the following example
                  </p><pre class="pre codeblock"><code>    @Override
    public Object deserialize(Writable w) throws SerDeException {
        Text rowText = (Text) w;
        List&lt;Text&gt; row = new ArrayList&lt;Text&gt;(columnNames.size());
        
        //default all values to null
        for(int i=0;i&lt;columnNames.size();i++){
        	row.add(null);
        }
        
        // Try parsing row into JSON object
        JsonNode recordNode = null;
        
        try {
        	String txt = rowText.toString().trim();
        	recordNode = jsonMapper.readTree(txt);
			row.set(columnNames.indexOf("id"), new Text(recordNode.get("id").getTextValue()));
			row.set(columnNames.indexOf("followers_count"), new Text(recordNode.get("followers_count").toString()));
			row.set(columnNames.indexOf("friends_count"), new Text(recordNode.get("friends_count").toString()));
			row.set(columnNames.indexOf("location"), new Text(recordNode.get("location").getTextValue()));
			row.set(columnNames.indexOf("user_id"), new Text(recordNode.get("user_id").toString()));
			
			Double longitude = recordNode.get("longitude").getDoubleValue();
			Double latitude = recordNode.get("latitude").getDoubleValue();
			
			//use the Spatial API to create the geometry
			JGeometry geom = JGeometry.createPoint(new double[]{
					longitude, 
					latitude}, 
					2, //dimensions
					8307 //SRID
					);
			//Transform the JGeometry to WKT
 			String geoWKT = new String(wkt.fromJGeometry(geom));
			row.set(columnNames.indexOf("geometry"), new Text(geoWKT));
        } catch (Exception e) {
            throw new SerDeException("Exception parsing JSON: " +e.getMessage(), e);
        }
	
        return row;
    }    
</code></pre><p>In the preceding example, to return the geometries in GeoJSON format, replace the following:</p><pre class="pre codeblock"><code>String geoWKT = new String(wkt.fromJGeometry(geom));
row.set(columnNames.indexOf("geometry"), new Text(geoWKT));
</code></pre><p>with this:</p><pre class="pre codeblock"><code>row.set(columnNames.indexOf("geometry"), new Text(geom.toGeoJson()));</code></pre><p>More SerDe examples to transform data in GeoJSON, WKT, or ESRI Shapefiles with the Spatial Java API are available in the folder: <code class="codeph">/opt/oracle/oracle-spatial-graph/spatial/vector/examples/hive/java/src/oracle/spatial/hadoop/vector/hive/java/src/serde</code></p>
                  <p>The following example queries the Hive table:</p><pre class="pre codeblock"><code>select ID, FOLLOWERS_COUNT, FRIENDS_COUNT, LOCATION, USER_ID, GEOMETRY from CUST_TWEETS_HIVE_TAB limit 10;</code></pre><p>The output looks like the following:</p><pre class="oac_no_warn" dir="ltr">6703	1	62	Hong Kong	3479846	POINT (114.18306 22.30693)
6702	57	166	Singapore	1765655	POINT (103.85387 1.29498)
</pre></div>
               <div>
                  <ul class="ullinks">
                     <li class="ulchildlink"><a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-5C2C58CE-84BA-42E7-BB99-011B366B5DA8">Creating Oracle External Tables for HDFS Files with Big Data SQL</a><br></li>
                     <li class="ulchildlink"><a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-7E915C08-D008-4750-8377-C7AD89C1FD88">Creating Oracle External Tables Using Hive Tables with Big Data SQL</a><br></li>
                  </ul>
                  <div class="familylinks">
                     <div class="parentlink">
                        <p><strong>Parent topic:</strong> <a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-77B38F53-21C9-4147-BE88-211A57E8A413" title="You can use Oracle Big Data Connectors to facilitate spatial data access between Big Data Spatial and Graph and Oracle Database.">Integrating Big Data Spatial and Graph with Oracle Database</a></p>
                     </div>
                  </div>
               </div>
               
               <div class="props_rev_3"><a id="GUID-5C2C58CE-84BA-42E7-BB99-011B366B5DA8" name="GUID-5C2C58CE-84BA-42E7-BB99-011B366B5DA8"></a><h4 id="BDSPA-GUID-5C2C58CE-84BA-42E7-BB99-011B366B5DA8" class="sect4"><span class="enumeration_section">3.4.1 </span>Creating Oracle External Tables for HDFS Files with Big Data SQL
                  </h4>
                  <div>
                     <p>You can create Oracle external tables for any kind of files in HDFS. The spatial information can be in a well known format or a custom format.</p>
                     <p>If the geometry format is not WKT or GeoJSON, then use one of the provided SerDe examples in the folder <code class="codeph">/opt/oracle/oracle-spatial-graph/spatial/vector/examples/hive/java/src/oracle/spatial/hadoop/vector/hive/java/src/serde</code>, or create a custom SerDe as in the example in <a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-F6BE6A07-F94F-4347-85E3-10818A48CBAA" title="To use Oracle SQL Connector for HDFS (OSCH) with files generated by Oracle Loader for Hadoop (OLH), you must understand how OLH is used to move data from HDFS to Oracle Database.">Using Oracle SQL Connector for HDFS with Files Generated by Oracle Loader for Hadoop</a>.
                     </p>
                     <p>After that, create an Oracle external table, as in the following example:</p><pre class="pre codeblock"><code>CREATE TABLE SAMPLE_TWEETS (id VARCHAR2(4000), 
  geometry VARCHAR2(4000), 
  followers_count VARCHAR2(4000), 
  friends_count VARCHAR2(4000), 
  location VARCHAR2(4000), user_id VARCHAR2(4000)) ORGANIZATION EXTERNAL
             (TYPE oracle_hdfs DEFAULT DIRECTORY DEFAULT_DIR
 ACCESS PARAMETERS (
  com.oracle.bigdata.rowformat: \
     SERDE 'mypackage.TweetsSerDe'
  com.oracle.bigdata.fileformat: \
     INPUTFORMAT 'oracle.spatial.hadoop.vector.geojson.mapred.GeoJsonInputFormat' \
     OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' \
  )
LOCATION ('/user/scott/simple_tweets_data/*.log'));
</code></pre><p>The table SAMPLE_TWEETS is now ready to query. It can be queried like any other table in the database. For example:</p><pre class="pre codeblock"><code>select count(*) from SAMPLE_TWEETS;</code></pre><p>You can perform spatial operations on that table, such as the following example to retrieve the users that are tweeting in a quarter-mile radius of a cinema:</p><pre class="pre codeblock"><code>select sdo_geom.SDO_DISTANCE(ci.geometry, SDO_GEOMETRY(tw.geometry, 8307), 0.5, 'UNIT=YARD'), ci.name, tw.user_id 
from CINEMA ci, SAMPLE_TWEETS tw where SDO_WITHIN_DISTANCE(ci.geometry, SDO_GEOMETRY(tw.geometry, 8307), 'DISTANCE=200 UNIT=MILE') = 'TRUE';
</code></pre><p>This information can be used further to customize advertising.</p>
                     <p>Note that the SRID of the geometries is 8307. Also, if the spatial data is in GeoJSON format, then the query should be as follows:</p><pre class="pre codeblock"><code>select sdo_geom.SDO_DISTANCE(ci.geometry, SDO_UTIL.FROM_GEOJSON(tw.geometry, '', 8307), 0.5, 'UNIT=YARD'), ci.name, tw.user_id 
from CINEMA ci, SAMPLE_TWEETS tw where SDO_WITHIN_DISTANCE(ci.geometry, SDO_UTIL.FROM_GEOJSON(tw.geometry, '', 8307), 'DISTANCE=200 UNIT=MILE') = 'TRUE';
</code></pre></div>
                  <div>
                     <div class="familylinks">
                        <div class="parentlink">
                           <p><strong>Parent topic:</strong> <a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-22143F85-39AA-4C28-8B57-D588AA1D9D8A" title="You can use Oracle Big Data SQL to facilitate spatial data access between HDFS and Oracle Database.">Integrating HDFS Spatial Data with Oracle Database Using Oracle Big Data SQL</a></p>
                        </div>
                     </div>
                  </div>
                  
               </div>
               <div class="props_rev_3"><a id="GUID-7E915C08-D008-4750-8377-C7AD89C1FD88" name="GUID-7E915C08-D008-4750-8377-C7AD89C1FD88"></a><h4 id="BDSPA-GUID-7E915C08-D008-4750-8377-C7AD89C1FD88" class="sect4"><span class="enumeration_section">3.4.2 </span>Creating Oracle External Tables Using Hive Tables with Big Data SQL
                  </h4>
                  <div>
                     <p>You can create Oracle external tables using Hive tables with Big Data SQL. The spatial information can be in a well known format or a custom format.</p>
                     <p>A Hive table used to create an Oracle external table must be created as described in <a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-F6BE6A07-F94F-4347-85E3-10818A48CBAA" title="To use Oracle SQL Connector for HDFS (OSCH) with files generated by Oracle Loader for Hadoop (OLH), you must understand how OLH is used to move data from HDFS to Oracle Database.">Using Oracle SQL Connector for HDFS with Files Generated by Oracle Loader for Hadoop</a>.
                     </p>
                     <p>Create an Oracle external table that can be created using the Hive table. For example:</p><pre class="pre codeblock"><code>CREATE TABLE SAMPLE_TWEETS (id VARCHAR2(4000),  geometry VARCHAR2(4000),  followers_count VARCHAR2(4000),  friends_count VARCHAR2(4000),  location VARCHAR2(4000), user_id VARCHAR2(4000))  ORGANIZATION EXTERNAL
(TYPE ORACLE_HIVE
 DEFAULT DIRECTORY DEFAULT_DIR 
 ACCESS PARAMETERS (
com.oracle.bigdata.cluster=cluster
com.oracle.bigdata.tablename=default.CUST_TWEETS_HIVE_TAB)
) PARALLEL 2 REJECT LIMIT UNLIMITED;
</code></pre><p>The table SAMPLE_TWEETS is now ready to query. It can be queried like any other table in the database. For example:</p><pre class="pre codeblock"><code>select count(*) from SAMPLE_TWEETS;</code></pre><p>You can perform spatial operations on that table, such as the following example to retrieve the users that are tweeting in a quarter-mile radius of a cinema:</p><pre class="pre codeblock"><code>select sdo_geom.SDO_DISTANCE(ci.geometry, SDO_GEOMETRY(tw.geometry, 8307), 0.5, 'UNIT=YARD'), ci.name, tw.user_id 
from CINEMA ci, SAMPLE_TWEETS tw where SDO_WITHIN_DISTANCE(ci.geometry, SDO_GEOMETRY(tw.geometry, 8307), 'DISTANCE=200 UNIT=MILE') = 'TRUE';
</code></pre><p>This information can be used further to customize advertising.</p>
                     <p>Note that the SRID of the geometries is 8307. Also, if the spatial data is in GeoJSON format, then the query should be as follows:</p><pre class="pre codeblock"><code>select sdo_geom.SDO_DISTANCE(ci.geometry, SDO_UTIL.FROM_GEOJSON(tw.geometry, '', 8307), 0.5, 'UNIT=YARD'), ci.name, tw.user_id 
from CINEMA ci, SAMPLE_TWEETS tw where SDO_WITHIN_DISTANCE(ci.geometry, SDO_UTIL.FROM_GEOJSON(tw.geometry, '', 8307), 'DISTANCE=200 UNIT=MILE') = 'TRUE';
</code></pre></div>
                  <div>
                     <div class="familylinks">
                        <div class="parentlink">
                           <p><strong>Parent topic:</strong> <a href="integrating-big-data-spatial-graph-with-oracle-database.html#GUID-22143F85-39AA-4C28-8B57-D588AA1D9D8A" title="You can use Oracle Big Data SQL to facilitate spatial data access between HDFS and Oracle Database.">Integrating HDFS Spatial Data with Oracle Database Using Oracle Big Data SQL</a></p>
                        </div>
                     </div>
                  </div>
                  
               </div>
            </div>
         </div>
      </article>
   </body>
</html>